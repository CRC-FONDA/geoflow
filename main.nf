nextflow.enable.dsl = 2

include { spat_lucas } from './nextflow-scripts/preprocess/preprocessing_workflows.nf'
include { explode_base_files } from './nextflow-scripts/preprocess/explode.nf'
include { build_vrt_stack } from './nextflow-scripts/preprocess/stack.nf'
include { mask_layer_stack } from './nextflow-scripts/preprocess/mask.nf'
include { scale_files } from './nextflow-scripts/aux/scale_raster.nf'
include { mask_and_scale } from './nextflow-scripts/preprocess/mask_and_scale.nf'
include { calculate_spectral_indices } from './nextflow-scripts/preprocess/indices.nf'
include { calc_stms_pr as stms_ls } from './nextflow-scripts/hl/stms.nf'
include { stack_generation } from './nextflow-subworkflows/stack_generation.nf'
include { ml_modeling } from './nextflow-subworkflows/modeling.nf'
include { build_class_vrt } from './nextflow-scripts/aux/build_outvrt.nf'

/* NOTE: This expects a cubed data provided or 'managed' by FORCE as input
 * - Flatten (partially) nested return from `Channel.fromFilePairs`
 * - Extract various information from the file path and file name, such as:
 *   - tile ID (generated by FORCE)
 *   - observation date
 *   - sensor name (long)
 *   - single letter sensor abbreviation
 *   - scene ID (i.e. file name of the input file, minus the product 'id'
 *     (such as 'BOA') and file type
 */
def prepare_channel = input -> {
	String reflectance_path = input[1][0]
	String quality_path = input[1][1]
	String tile = reflectance_path.split('/')[-2]
	String scene = input[0]
	String date = scene.split('_')[0]
	String sensor = scene.split('_')[-1]
	String sensor_abbreviation = sensor[0]

	return [tile, date, scene, sensor, sensor_abbreviation, reflectance_path, quality_path]
}

def insert_stm_frame = input -> {
	String stm_frame = input[-2] + '_' + input[-1]

	return [input[0], stm_frame, input[1], input[2], input[3], input[4], input[5], input[6], input[7]]
}

def is_landsat = input -> {
    return input[4] == 'L'
}

workflow {
	Channel
		.fromPath([params.lucas_survey, params.lucas_geom], type: 'file')
		.concat(Channel.of(params.lucas_query, params.lucas_epsg))
		.collect()
		.set({ lucas })

	spat_lucas(lucas)

	Channel
		.of(params.spectral_indices_mapping)
		.set({ spectral_indices })

	Channel
		.of(params.calculate_stms)
		.set({ stm_choices })

	Channel
		.of(params.stm_band_mapping_sentinel)
		.mix( spectral_indices )
		.flatMap()
		.combine( stm_choices )
		.set({ stm_combination_sentinel })

	Channel
		.of(params.stm_band_mapping_landsat)
		.mix( spectral_indices )
		.flatMap()
		.set({ stm_combination_landsat })

	Channel
		.fromFilePairs(params.input_cube)
		.map({ it.unique() }) // is this sufficient to filter the list?
		.map({ prepare_channel(it) })
		.filter({ is_landsat(it) })
		.filter({ it[1] >= params.processing_timeframe["START"] && it[1] <= params.processing_timeframe["END"] })
		.set({ ch_dataP })

	//mask_layer_stack(ch_dataP)

	//scale_files(
	//	mask_layer_stack
	//	.out
	//)

	mask_and_scale(ch_dataP)

	mask_and_scale
	    .out
	    .set({ ch_base_files })

	calculate_spectral_indices(
		ch_base_files
			.combine(
				spectral_indices.flatMap()
			)	
	)

	explode_base_files(ch_base_files)

	Channel
		.empty()
		.mix(calculate_spectral_indices.out, explode_base_files.out)
		.combine(params.stm_timeframes) // inserts start and end time as flat elements on the end
		// -> tile, date, scene, sensor, sensor_abbr, BOA, QAI, IDX/SL-VRT, {STM_start, STM_end}
		.map({ insert_stm_frame(it) })
		.filter({ it[2] >= it[1].split('_')[0] && it[2] <= it[1].split('_')[1] }) // filter observations where capture date falls within STM timeframe
		.groupTuple(by: [0, 1]) // group by tile and STM period
		// [tile, stm period, unique BOA, [indices and flat BOAs]]
		.map({ [it[0], it[1], it[6].unique({ a, b -> a.name <=> b.name }), it[8].flatten()] })
		.set({ ch_group_stacked_raster })

	/* conceptually, new chunk as per proposed flow chart */

	stms_ls(
	    ch_group_stacked_raster
	        .combine(stm_combination_landsat)
	        .combine(stm_choices)
	)

	stack_generation(stms_ls)

	stack_generation
		.out
		.set({ feature_stack })

	ml_modeling(feature_stack, feature_stack, spat_lucas)

	build_class_vrt(
		ml_modeling
			.out
			.collect()
	)

}

